{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e01911-942c-410c-b642-858b394f9234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75106786-8775-4899-a029-257f8d4eb587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaValueError: could not parse 'name: structcoder' in: structcoder.yml\n",
      "\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: structcoder\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env create -n structcoder --file structcoder.yml\n",
    "!conda activate structcoder\n",
    "!conda install -c anaconda ipykernel\n",
    "!python3 -m ipykernel install --user --name=structcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from parser import remove_comments_and_docstrings\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index, \n",
    "                   detokenize_code, tree_to_token_nodes)\n",
    "from tree_sitter import Language, Parser\n",
    "import pickle\n",
    "import os\n",
    "from transformers import RobertaTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_pt_dataset(max_samples_per_split=None):\n",
    "    dataset = load_dataset('code_search_net')\n",
    "    rows = []\n",
    "    \n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        num_samples_in_split = len(dataset[split])\n",
    "        indices = list(range(num_samples_in_split))\n",
    "        if (max_samples_per_split is not None) and (num_samples_in_split>max_samples_per_split):\n",
    "            indices = list(map(int, np.random.choice(indices, max_samples_per_split, replace=False)))\n",
    "        pbar = tqdm(indices)\n",
    "        pbar.set_description('Reading split='+split)\n",
    "        \n",
    "        for i in pbar:\n",
    "            sample = dataset[split][i]\n",
    "            rows.append([sample['func_code_string'], sample['language'], \n",
    "                         sample['func_documentation_string']])\n",
    "            \n",
    "    return pd.DataFrame(rows, columns=['code', 'lang', 'text'])\n",
    "\n",
    "\n",
    "def add_php_ends(code):\n",
    "    if not(code.startswith('<?php')):\n",
    "        code=\"<?php \"+code\n",
    "    if not(code.endswith('?>')):\n",
    "        code=code+\"?>\" \n",
    "    return code\n",
    "\n",
    "\n",
    "def print_lang_dist(langs, total=None):\n",
    "    if total is None:\n",
    "        total = len(langs)\n",
    "    vc = pd.value_counts(langs)\n",
    "    display(pd.DataFrame({'lang':vc.index, 'count':vc.values, 'perc':vc.values/total*100}))\n",
    "\n",
    "    \n",
    "def get_tokenizer_chars():\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    tokenizer_chars = []\n",
    "    for i in range(tokenizer.vocab_size):\n",
    "        token = tokenizer.decode(i)\n",
    "        if len(token)==1:\n",
    "            tokenizer_chars.append(token)\n",
    "    tokenizer_chars = [c for c in tokenizer_chars if c!='�']\n",
    "    return tokenizer_chars\n",
    "    \n",
    "\n",
    "def preprocess(data):\n",
    "    codes = []\n",
    "    failed_count = 0\n",
    "    failed_langs = []\n",
    "    rows = []\n",
    "    tokenizer_chars = get_tokenizer_chars()\n",
    "    pbar = tqdm(data.itertuples())\n",
    "    for row in pbar:\n",
    "        code = row.code.strip().replace('▁', '_').replace('\\r\\n', '\\n') # step 1\n",
    "        code = ''.join(filter(lambda c:c in tokenizer_chars, code)) # step 2\n",
    "        if row.lang==\"php\":\n",
    "            code = add_php_ends(code) # step 3\n",
    "        try:\n",
    "            code = remove_comments_and_docstrings(code, row.lang) # step 4\n",
    "        except:\n",
    "            failed_count += 1\n",
    "            failed_langs.append(row.lang)\n",
    "            pbar.set_description('failed_count='+str(failed_count))\n",
    "            continue\n",
    "        rows.append([code, row.lang, row.text.strip()])\n",
    "    if failed_count:\n",
    "        print ('Distribution of languages among failed samples for remove_comments_and_docstrings()')\n",
    "        print_lang_dist(failed_langs)\n",
    "    data = pd.DataFrame(rows, columns=['code', 'lang', 'text'])\n",
    "    print ('Distribution of languages after removing samples failing remove_comments_and_docstrings()')\n",
    "    print_lang_dist(data.lang)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_structure(code, parser):\n",
    "    # ast\n",
    "    tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "    root_node = tree.root_node  \n",
    "    ast_token_nodes = tree_to_token_nodes(root_node) # leaves\n",
    "    \n",
    "    # dfg\n",
    "    tokens_index = [(node.start_point, node.end_point) for node in ast_token_nodes]\n",
    "    code=code.split('\\n')\n",
    "    code_tokens=[index_to_code_token(x,code) for x in tokens_index] \n",
    "    index_to_code={index:(idx,code_) for idx,(index,code_) in enumerate(zip(tokens_index,code_tokens))}\n",
    "    try:\n",
    "        DFG,_ = parser[1](root_node,index_to_code,{}) \n",
    "    except:\n",
    "        DFG = []\n",
    "    for d in DFG:\n",
    "        assert (d[2]=='comesFrom' or d[2]=='computedFrom')\n",
    "    DFG = [(d[1], d[4]) for d in DFG if (len(d[4])>0)] # left comes from right\n",
    "    return code_tokens, ast_token_nodes, DFG\n",
    "\n",
    "\n",
    "def format_node_ranges(code, nodes):\n",
    "    line_lens = [len(line)+1 for line in code.split('\\n')]\n",
    "    line_starts = [0] + list(np.cumsum(line_lens))\n",
    "    return [(line_starts[node.start_point[0]]+node.start_point[1],\n",
    "             line_starts[node.end_point[0]]+node.end_point[1]) for node in nodes]\n",
    "\n",
    "    \n",
    "def length_stats(s, title=None):\n",
    "    try:\n",
    "        if type(s.iloc[0])==str: # a list encoded as str\n",
    "            lens = s.apply(lambda x:x.count(',')+1)\n",
    "        else: # a list\n",
    "            lens = s.apply(len)\n",
    "    except:\n",
    "        lens = s # s contains lengths\n",
    "    y = np.arange(100)\n",
    "    x = lens.quantile(y/100)\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def add_structure(data):\n",
    "    dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'go':DFG_go,\n",
    "    'ruby':DFG_ruby\n",
    "    }\n",
    "\n",
    "    parsers={}        \n",
    "    for lang in dfg_function:\n",
    "        LANGUAGE = Language('parser/my-languages2.so', lang)\n",
    "        parser = Parser()\n",
    "        parser.set_language(LANGUAGE) \n",
    "        parser = [parser,dfg_function[lang]]    \n",
    "        parsers[lang]= parser\n",
    "        \n",
    "    ast_leaf_tokens, ast_leaves, ast_leaf_ranges, dfg_edges = [], [], [], []\n",
    "    for row in tqdm(data.itertuples()):\n",
    "        curr_code_tokens, curr_ast_leaves, curr_dfg_edges = extract_structure(row.code, parsers[row.lang])\n",
    "        ast_leaf_tokens.append(curr_code_tokens)\n",
    "        ast_leaves.append(curr_ast_leaves)\n",
    "        ast_leaf_ranges.append(format_node_ranges(row.code, curr_ast_leaves))\n",
    "        dfg_edges.append(curr_dfg_edges)\n",
    "        \n",
    "    data['ast_leaves'] = ast_leaves # list of leaf nodes\n",
    "    data['dfg_edges'] = dfg_edges # list of \"left leaf node index comes from right leaf nodes indices\"\n",
    "    data['ast_leaf_tokens'] = ast_leaf_tokens # list of code substrings corresponding to each leaf\n",
    "    data['ast_leaf_ranges'] = ast_leaf_ranges # list of (start,end) in code for each leaf node\n",
    "    \n",
    "    print ('Distribution of languages among codes with failed/empty DFG')\n",
    "    print_lang_dist(data.loc[data['dfg_edges'].apply(len)==0].lang, total=len(data))\n",
    "    \n",
    "    \n",
    "def tokenize_codes_texts(texts, batch_size=1024):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    N = len(texts)\n",
    "    tokenized_texts = []\n",
    "    for start in tqdm(range(0, len(texts),batch_size)):\n",
    "        tokenized_texts += tokenizer(texts[start:start+batch_size]).input_ids\n",
    "    return tokenized_texts\n",
    "\n",
    "    \n",
    "def get_code_tokens_ranges(data):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    match  = {lang:[0,0] for lang in data['lang'].unique()}\n",
    "    pbar = tqdm(data.itertuples())\n",
    "    ranges = []\n",
    "    \n",
    "    for row in pbar:\n",
    "        code_tokens = [tokenizer.decode(ct) for ct in row.code_tokens][1:-1] # 1:-1 to remove <s> and </s>\n",
    "        code2 = ''.join(code_tokens) # misses some spaces that are in row.code\n",
    "        code = row.code\n",
    "        \n",
    "        # map each position in code2 to a position in code\n",
    "        code2_to_code = []\n",
    "        j=0\n",
    "        for i in range(len(code2)):\n",
    "            if code2[i]==code[j]:\n",
    "                code2_to_code.append(j)\n",
    "                j += 1\n",
    "            elif code2[i]==code[j+1]: # if code2 missed a space\n",
    "                code2_to_code.append(j+1)\n",
    "                j += 2\n",
    "            else:\n",
    "                raise Exception('Character \"'+code2[i]+'\" from tokenized code not found in code.')\n",
    "            \n",
    "        # map each code token to a range in code\n",
    "        code2_idx = 0\n",
    "        curr_ranges = []\n",
    "        for ct in code_tokens:\n",
    "            s,e = code2_idx, code2_idx+len(ct)\n",
    "            code2_idx = e\n",
    "            curr_ranges.append((min(code2_to_code[s:e]),1+max(code2_to_code[s:e])))\n",
    "        ranges.append([None]+curr_ranges+[None]) # first and last for <s> and </s>\n",
    "        \n",
    "    data['code_tokens_ranges'] = ranges\n",
    "    \n",
    "    \n",
    "def overlap(s1,e1,s2,e2):\n",
    "    return s1<=s2<e1 or s2<=s1<e2\n",
    "    \n",
    "def get_leaf_code_token_indices(data):\n",
    "    ast_leaf_token_idxs = []\n",
    "    for row in tqdm(data.itertuples()):\n",
    "        j = 1\n",
    "        ast_leaf_token_idxs.append([])\n",
    "        code_tokens_last_idx = len(row.code_tokens)-1\n",
    "        for s,e in row.ast_leaf_ranges:\n",
    "            if s==e: # there are leaves with start_point=end_point\n",
    "                ast_leaf_token_idxs[-1].append([])\n",
    "                continue\n",
    "            while not(overlap(s,e,row.code_tokens_ranges[j][0],row.code_tokens_ranges[j][1])):\n",
    "                j += 1\n",
    "            jj = j\n",
    "            curr_leaf_token_idxs = []\n",
    "            while overlap(s,e,row.code_tokens_ranges[jj][0],row.code_tokens_ranges[jj][1]):\n",
    "                curr_leaf_token_idxs.append(jj)\n",
    "                jj += 1\n",
    "                if jj==code_tokens_last_idx:\n",
    "                    break\n",
    "            ast_leaf_token_idxs[-1].append(curr_leaf_token_idxs)\n",
    "    data['ast_leaf_code_token_idxs'] = ast_leaf_token_idxs\n",
    "    \n",
    "\n",
    "def get_lr_path(leaf):\n",
    "    path = [leaf]\n",
    "    while path[-1].parent is not None:\n",
    "        path.append(path[-1].parent)\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_ll_sim(p1, p2): \n",
    "    common = 1\n",
    "    for i in range(2, min(len(p1), len(p2))+1):\n",
    "        if p1[-i]==p2[-i]:\n",
    "            common += 1\n",
    "        else:\n",
    "            break\n",
    "    return common*common / (len(p1)*len(p2))   \n",
    "\n",
    "\n",
    "def process_dfg_edges(data):\n",
    "    dfg_node_code_token_idxs = []\n",
    "    dfg_edges = []\n",
    "    for row in tqdm(data.itertuples()):\n",
    "        if len(row.dfg_edges)>0:\n",
    "            nodes = sorted(list(set(np.concatenate([[left]+right for left,right in row.dfg_edges]))))\n",
    "        else:\n",
    "            nodes = []\n",
    "        node_to_idx = {k:i for i,k in enumerate(nodes)}\n",
    "        dfg_node_code_token_idxs.append( [row.ast_leaf_code_token_idxs[i] for i in nodes] )\n",
    "        dfg_edges.append( [(node_to_idx[left], [node_to_idx[r] for r in right]) for left,right in row.dfg_edges] )\n",
    "    data['dfg_edges'] = dfg_edges\n",
    "    data['dfg_node_code_token_idxs'] = dfg_node_code_token_idxs\n",
    "    \n",
    "def get_ast_lr_paths_and_ll_sim(data):\n",
    "    sims = []\n",
    "    lr_paths = []\n",
    "    all_node_types = set()\n",
    "    for i,row in tqdm(enumerate(data.itertuples())):\n",
    "        L = min(len(row.ast_leaves), 512)\n",
    "        curr_paths = [get_lr_path(leaf) for leaf in row.ast_leaves]\n",
    "        curr_sims = np.ones((L,L))\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                curr_sims[i,j] = curr_sims[j,i] = get_ll_sim(curr_paths[i], curr_paths[j])\n",
    "        sims.append(';'.join([','.join(list(map(str,row))) for row in curr_sims]))\n",
    "        lr_paths.append([[node.type for node in path] for path in curr_paths])\n",
    "        all_node_types.update(set(np.concatenate(lr_paths[-1])))\n",
    "    data.drop(columns=['ast_leaves'], inplace=True)\n",
    "    data['ll_sims'] = sims\n",
    "    data['lr_paths_types'] = lr_paths\n",
    "    return all_node_types\n",
    "\n",
    "def parse_list_of_lists(s, type_=int):\n",
    "    list_of_lists = s[1:-2].split('], ')\n",
    "    if type_==str:\n",
    "        list_of_lists = [[t[1:-1].replace('\\\\n','\\n').replace('\\\\\\\\','\\\\') for  t in x[1:].split(', ')] \\\n",
    "                         for x in list_of_lists]\n",
    "    elif type_==int:\n",
    "        list_of_lists = [[int(t) for  t in x[1:].split(', ')] for x in list_of_lists]\n",
    "    else:\n",
    "        raise Exception('Unknown value for type_')\n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcc6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples_per_split, num_rows_per_file = None, 10000\n",
    "# num_samples_per_split, num_rows_per_file = 100, 200 # for debugging\n",
    "\n",
    "np.random.seed(10)\n",
    "data = read_pt_dataset(num_samples_per_split) # columns: code, text, lang\n",
    "data = preprocess(data)\n",
    "add_structure(data) # columns: ast_leaves, dfg_edges, ast_leaf_tokens, ast_leaf_ranges\n",
    "data['code_tokens'] = tokenize_codes_texts(list(data['code']))\n",
    "data['text_tokens'] = tokenize_codes_texts(list(data['text']))\n",
    "length_stats(data['code_tokens'], 'Distribution of #code_tokens')\n",
    "length_stats(data['text_tokens'], 'Distribution of #text_tokens')\n",
    "get_code_tokens_ranges(data) # columns: code_token_ranges -> list of (start,end) one for each code_token\n",
    "data.drop(columns=['code', 'text'], inplace=True)\n",
    "get_leaf_code_token_indices(data)\n",
    "data.drop(columns=['ast_leaf_tokens', 'ast_leaf_ranges', 'code_tokens_ranges'], inplace=True)\n",
    "for col in ['code_tokens', 'text_tokens']:\n",
    "    data[col] = data[col].progress_apply(lambda l:','.join(list(map(str,l))))\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# columns -> ['lang', 'ast_leaves', 'dfg_edges', 'code_tokens', 'text_tokens', 'ast_leaf_code_token_idxs']\n",
    "\n",
    "# do memory intensive part in chunks\n",
    "save_dir = 'data/pretrain/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "all_node_types = set()\n",
    "for start in range(0,len(data),num_rows_per_file):\n",
    "    print ('Working on from_'+str(start)+'.parquet')\n",
    "    sub_data = data.iloc[start:start+num_rows_per_file].copy() # copy so that edits are not on data\n",
    "    sub_node_types = get_ast_lr_paths_and_ll_sim(sub_data)\n",
    "    all_node_types.update(sub_node_types)\n",
    "    process_dfg_edges(sub_data)\n",
    "    sub_data = sub_data[['code_tokens', 'text_tokens', 'lang', \n",
    "                         'ast_leaf_code_token_idxs', 'll_sims', 'lr_paths_types', \n",
    "                         'dfg_node_code_token_idxs', 'dfg_edges']]\n",
    "    for col in ['ast_leaf_code_token_idxs', 'lr_paths_types', 'dfg_node_code_token_idxs', 'dfg_edges']:\n",
    "        sub_data[col] = sub_data[col].apply(str)\n",
    "    sub_data.to_parquet(save_dir+'from_'+str(start)+'.parquet', engine='fastparquet', row_group_offsets=100)\n",
    "del data\n",
    "    \n",
    "# convert node types to indices\n",
    "all_node_types = sorted(list(all_node_types))\n",
    "node_type_to_ind = {t:i for i,t in enumerate(all_node_types)}\n",
    "pickle.dump(all_node_types, open(save_dir+'all_node_types.pkl', 'wb'))\n",
    "\n",
    "for filename in tqdm(os.listdir(save_dir)):\n",
    "    if filename.startswith('from_'):\n",
    "        sub_data = pd.read_parquet(save_dir+filename, engine='fastparquet')\n",
    "        sub_data['lr_paths_types'] = sub_data['lr_paths_types'].apply(\n",
    "                        lambda s:str([[node_type_to_ind[t] for t in path] \n",
    "                                      for path in parse_list_of_lists(s, type_=str)]))\n",
    "        sub_data.to_parquet(save_dir+filename, engine='fastparquet', row_group_offsets=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94010f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce memory taken by ll_sims column by storing only upper triangles w/o diagnoals.\n",
    "def upper_triangle(s):\n",
    "    rows = s.split(';')[:-1] \n",
    "    s = ''\n",
    "    for i,row in enumerate(rows):\n",
    "        s += ','.join(row.split(',')[i+1:]) + ';'\n",
    "    return s[:-1]\n",
    "pbar = tqdm(os.listdir(save_dir))\n",
    "for filename in pbar:\n",
    "    pbar.set_description(filename)\n",
    "    if filename.startswith('from_'):\n",
    "        sub_data = pd.read_parquet(save_dir+filename, engine='fastparquet')\n",
    "        sub_data['ll_sims'] = sub_data['ll_sims'].apply(upper_triangle)\n",
    "        sub_data.to_parquet(save_dir+filename, engine='fastparquet', row_group_offsets=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41408c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def some_more_stats(data):\n",
    "    data['lr_paths_types'] = data['lr_paths_types'].progress_apply(lambda s:parse_list_of_lists(s, type_=int))\n",
    "    node_types= pickle.load(open('data/pretrain/all_node_types.pkl','rb'))\n",
    "    if 'ERROR' in node_types:\n",
    "        error_node_idx = node_types.index('ERROR')\n",
    "        num_error_nodes = data['lr_paths_types'].apply(lambda paths:np.mean([(np.array(path)==error_node_idx).max() \n",
    "                                                                      for path in paths]))\n",
    "        print ('Distrubution of fraction of leaf-root paths with ERROR node in one code')\n",
    "        length_stats(num_error_nodes)\n",
    "    print ('Distrubution of AST depth')\n",
    "    length_stats(data['lr_paths_types'].apply(lambda paths:max([len(p) for p in paths])))        \n",
    "        \n",
    "    print ('Distrubution of # ast leaves per code')\n",
    "    length_stats(data['ast_leaf_code_token_idxs'].apply(lambda s:1+s.count('],')))\n",
    "    print ('Distrubution of # dfg nodes per code')\n",
    "    length_stats(data['dfg_node_code_token_idxs'].apply(lambda s:1+s.count('],')))\n",
    "    print ('Distrubution of # dfg edges per code')\n",
    "    def num_dfg_edges(s):\n",
    "        if s=='[]':\n",
    "            return 0\n",
    "        return sum([t.split(', ',maxsplit=1)[1].count(',')+1 for t in s[1:-2].split('),')])\n",
    "    length_stats(data['dfg_edges'].apply(num_dfg_edges))\n",
    "    \n",
    "data = []\n",
    "save_dir = 'data/pretrain/'\n",
    "for filename in tqdm(os.listdir(save_dir)):\n",
    "    if filename.startswith('from_'):\n",
    "        sub_data = pd.read_parquet(save_dir+filename, engine='fastparquet')\n",
    "        data.append(sub_data)\n",
    "        \n",
    "data = pd.concat(data)\n",
    "    \n",
    "some_more_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe5e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "getsizeof(data)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca808d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print (col, getsizeof(data[col])/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f24be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d18b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74151ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed4583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38bdf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605eadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d694f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa8843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ecdb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cded0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23ec0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHP and JS keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__halt_compiler\n",
      "abstract\n",
      "and\n",
      "array\n",
      "as\n",
      "break\n",
      "callable\n",
      "case\n",
      "catch\n",
      "class\n",
      "clone\n",
      "const\n",
      "continue\n",
      "declare\n",
      "default\n",
      "die\n",
      "do\n",
      "echo\n",
      "else\n",
      "elseif\n",
      "empty\n",
      "enddeclare\n",
      "endfor\n",
      "endforeach\n",
      "endif\n",
      "endswitch\n",
      "endwhile\n",
      "eval\n",
      "exit\n",
      "extends\n",
      "final\n",
      "for\n",
      "foreach\n",
      "function\n",
      "global\n",
      "goto\n",
      "if\n",
      "implements\n",
      "include\n",
      "include_once\n",
      "instanceof\n",
      "insteadof\n",
      "interface\n",
      "isset\n",
      "list\n",
      "namespace\n",
      "new\n",
      "or\n",
      "print\n",
      "private\n",
      "protected\n",
      "public\n",
      "require\n",
      "require_once\n",
      "return\n",
      "static\n",
      "switch\n",
      "throw\n",
      "trait\n",
      "try\n",
      "unset\n",
      "use\n",
      "var\n",
      "while\n",
      "xor\n"
     ]
    }
   ],
   "source": [
    "# php\n",
    "for i in ['__halt_compiler', 'abstract', 'and', 'array', 'as', 'break', 'callable', 'case', 'catch', 'class', 'clone', 'const', 'continue', 'declare', 'default', 'die', 'do', 'echo', 'else', 'elseif', 'empty', 'enddeclare', 'endfor', 'endforeach', 'endif', 'endswitch', 'endwhile', 'eval', 'exit', 'extends', 'final', 'for', 'foreach', 'function', 'global', 'goto', 'if', 'implements', 'include', 'include_once', 'instanceof', 'insteadof', 'interface', 'isset', 'list', 'namespace', 'new', 'or', 'print', 'private', 'protected', 'public', 'require', 'require_once', 'return', 'static', 'switch', 'throw', 'trait', 'try', 'unset', 'use', 'var', 'while', 'xor']:\n",
    "    print (i)\n",
    "# JS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeBLEU for CoST dataset, StrcuctCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "java python === CodeBLEU score:   62.68  56.01\n",
      "java c_sharp === CodeBLEU score:   87.37  85.41\n",
      "java javascript === CodeBLEU score:   72.5  70.36\n",
      "java php === CodeBLEU score: ____  ____\n",
      "\n",
      "\n",
      "python java === CodeBLEU score:   59.54  57.56\n",
      "python c_sharp === CodeBLEU score: ____  ____\n",
      "python javascript === CodeBLEU score: ____  ____\n",
      "python php === CodeBLEU score: ____  ____\n",
      "\n",
      "\n",
      "c_sharp java === CodeBLEU score:   88.2  83.26\n",
      "c_sharp python === CodeBLEU score: ____  ____\n",
      "c_sharp javascript === CodeBLEU score: ____  ____\n",
      "c_sharp php === CodeBLEU score: ____  ____\n",
      "\n",
      "\n",
      "javascript java === CodeBLEU score:   71.95  69.61\n",
      "javascript python === CodeBLEU score: ____  ____\n",
      "javascript c_sharp === CodeBLEU score: ____  ____\n",
      "javascript php === CodeBLEU score: ____  ____\n",
      "\n",
      "\n",
      "php java === CodeBLEU score: ____  ____\n",
      "php python === CodeBLEU score: ____  ____\n",
      "php c_sharp === CodeBLEU score: ____  ____\n",
      "php javascript === CodeBLEU score: ____  ____\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import os\n",
    "\n",
    "    \n",
    "for l1 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "    print ('\\n')\n",
    "    for l2 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "        if l1==l2:\n",
    "            continue\n",
    "    \n",
    "        model_dir = '../saved_models/translation/'+l1+'-'+l2+'_60000_bs32_ep100/'\n",
    "        ref = model_dir + 'test_0.gold'\n",
    "        hyp = model_dir + 'test_0.output'\n",
    "        lang = l2\n",
    "        \n",
    "        if not (os.path.exists(ref)):\n",
    "            print(l1, l2, '=== CodeBLEU score: ____  ____')\n",
    "            continue\n",
    "            \n",
    "        pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "        hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "        for i in range(len(pre_references)):\n",
    "                assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "        references = []\n",
    "        for i in range(len(hypothesis)):\n",
    "            ref_for_instance = []\n",
    "            for j in range(len(pre_references)):\n",
    "                ref_for_instance.append(pre_references[j][i])\n",
    "            references.append(ref_for_instance)\n",
    "        assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "        # calculate ngram match (BLEU)\n",
    "        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "        # calculate weighted ngram match\n",
    "        keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "        def make_weights(reference_tokens, key_word_list):\n",
    "            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                    for token in reference_tokens}\n",
    "        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "        # calculate syntax match\n",
    "        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "        # calculate dataflow match\n",
    "        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    #     print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "    #                             format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "    #                                    round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "        code_bleu_score = (ngram_match_score\\\n",
    "                            + weighted_ngram_match_score\\\n",
    "                            + syntax_match_score\\\n",
    "                            + dataflow_match_score)/4\n",
    "\n",
    "        bleu_score = open(model_dir+'log.csv').readlines()[-7].split('=')[1].replace('\\n','')\n",
    "        print(l1, l2, '=== CodeBLEU score: ', bleu_score, round(code_bleu_score*100,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphcodebert-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "java python === CodeBLEU score:  0.5119469929533536 47.39\n",
      "java c_sharp === CodeBLEU score:  0.8520371684760304 80.44\n",
      "java javascript === CodeBLEU score:  0.5945075264446041 56.26\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "java php === CodeBLEU score:  0.45841186968576275 48.07\n",
      "\n",
      "\n",
      "python java === CodeBLEU score:  0.41527638408493883 41.15\n",
      "python c_sharp === CodeBLEU score:  0.4521564434507671 44.36\n",
      "python javascript === CodeBLEU score:  0.45860661093308885 44.92\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "python php === CodeBLEU score:  0.40521879002115097 45.35\n",
      "\n",
      "\n",
      "c_sharp java === CodeBLEU score:  0.8321075571550849 77.38\n",
      "c_sharp python === CodeBLEU score:  0.49574574176178726 46.18\n",
      "c_sharp javascript === CodeBLEU score:  0.5991425377230587 56.71\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "c_sharp php === CodeBLEU score:  0.45224173499250025 47.79\n",
      "\n",
      "\n",
      "javascript java === CodeBLEU score:  0.5678193281231351 53.54\n",
      "javascript python === CodeBLEU score:  0.4967219953030262 46.13\n",
      "javascript c_sharp === CodeBLEU score:  0.6186598361402048 58.97\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "javascript php === CodeBLEU score:  0.47136455676571337 48.71\n",
      "\n",
      "\n",
      "php java === CodeBLEU score:  0.3461219577011181 36.0\n",
      "php python === CodeBLEU score:  0.3675490505383789 38.27\n",
      "php c_sharp === CodeBLEU score:  0.38908025756031345 39.27\n",
      "php javascript === CodeBLEU score:  0.4036087174388778 39.44\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "\n",
    "\n",
    "    \n",
    "for l1 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "    print ('\\n')\n",
    "    for l2 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "        if l1==l2:\n",
    "            continue\n",
    "    \n",
    "        model_dir = '../../GraphCodeBERT/translation/saved_models/'+l1+'-'+l2+'/'\n",
    "        ref = model_dir + 'test_0.gold'\n",
    "        hyp = model_dir + 'test_0.output'\n",
    "        lang = l2\n",
    "\n",
    "        pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "        hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "        for i in range(len(pre_references)):\n",
    "                assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "        references = []\n",
    "        for i in range(len(hypothesis)):\n",
    "            ref_for_instance = []\n",
    "            for j in range(len(pre_references)):\n",
    "                ref_for_instance.append(pre_references[j][i])\n",
    "            references.append(ref_for_instance)\n",
    "        assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "        # calculate ngram match (BLEU)\n",
    "        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "        # calculate weighted ngram match\n",
    "        keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "        def make_weights(reference_tokens, key_word_list):\n",
    "            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                    for token in reference_tokens}\n",
    "        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "        # calculate syntax match\n",
    "        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "        # calculate dataflow match\n",
    "        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    #     print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "    #                             format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "    #                                    round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "        code_bleu_score = (ngram_match_score\\\n",
    "                            + weighted_ngram_match_score\\\n",
    "                            + syntax_match_score\\\n",
    "                            + dataflow_match_score)/4\n",
    "\n",
    "        print(l1, l2, '=== CodeBLEU score: ', ngram_match_score, round(code_bleu_score*100,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost - plbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "java python === CodeBLEU score:  62.37 57.18\n",
      "java c_sharp === CodeBLEU score:  88.25 80.23\n",
      "java javascript === CodeBLEU score:  79.31 77.45\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "java php === CodeBLEU score:  72.33 61.31\n",
      "====================================================================================================\n",
      "python java === CodeBLEU score:  33.82 31.97\n",
      "python c_sharp === CodeBLEU score:  69.39 64.22\n",
      "python javascript === CodeBLEU score:  73.55 68.89\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "python php === CodeBLEU score:  -1 59.27\n",
      "====================================================================================================\n",
      "c_sharp java === CodeBLEU score:  73.56 67.59\n",
      "c_sharp python === CodeBLEU score:  29.15 33.93\n",
      "c_sharp javascript === CodeBLEU score:  55.81 49.37\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "c_sharp php === CodeBLEU score:  25.89 38.28\n",
      "====================================================================================================\n",
      "javascript java === CodeBLEU score:  64.57 55.04\n",
      "javascript python === CodeBLEU score:  41.0 42.98\n",
      "javascript c_sharp === CodeBLEU score:  76.05 72.74\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "javascript php === CodeBLEU score:  78.24 64.16\n",
      "====================================================================================================\n",
      "php java === CodeBLEU score:  31.0 29.66\n",
      "php python === CodeBLEU score:  20.61 24.22\n",
      "php c_sharp === CodeBLEU score:  74.44 69.27\n",
      "php javascript === CodeBLEU score:  36.98 39.41\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import os\n",
    "\n",
    "dir_dict = {'javascript':'Javascript', 'java':'Java', 'c_sharp':'C#', 'php':'PHP', 'python':'Python'}\n",
    "end_dict = {'javascript':'js', 'java':'java', 'c_sharp':'cs', 'php':'php', 'python':'py'}\n",
    "    \n",
    "for l1 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "    print ('='*100)\n",
    "    for l2 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "        if l1==l2:\n",
    "            continue\n",
    "    \n",
    "        model_dir = '../../PLBART/scripts/plbart_csnet/saved_models/'+l1+'-'+l2+'/'\n",
    "\n",
    "        ref = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l1]+'-'+dir_dict[l2]+'/' + \\\n",
    "                'test-'+dir_dict[l1]+'-'+dir_dict[l2]+'-tok.'+end_dict[l2]\n",
    "        if not(os.path.exists(ref)):\n",
    "            ref = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l2]+'-'+dir_dict[l1]+'/' + \\\n",
    "                'test-'+dir_dict[l2]+'-'+dir_dict[l1]+'-tok.'+end_dict[l2]\n",
    "        hyp = model_dir + 'output.hyp'\n",
    "        lang = l2\n",
    "        \n",
    "        \n",
    "        if not(os.path.exists(hyp)):\n",
    "            print(l1, l2, '=== CodeBLEU score: _____ ______')\n",
    "            continue\n",
    "\n",
    "        pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "        hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "        for i in range(len(pre_references)):\n",
    "                assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "        references = []\n",
    "        for i in range(len(hypothesis)):\n",
    "            ref_for_instance = []\n",
    "            for j in range(len(pre_references)):\n",
    "                ref_for_instance.append(pre_references[j][i])\n",
    "            references.append(ref_for_instance)\n",
    "        assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "        # calculate ngram match (BLEU)\n",
    "        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "        # calculate weighted ngram match\n",
    "        keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "        def make_weights(reference_tokens, key_word_list):\n",
    "            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                    for token in reference_tokens}\n",
    "        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "        # calculate syntax match\n",
    "        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "        # calculate dataflow match\n",
    "        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    #     print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "    #                             format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "    #                                    round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "        code_bleu_score = (ngram_match_score\\\n",
    "                            + weighted_ngram_match_score\\\n",
    "                            + syntax_match_score\\\n",
    "                            + dataflow_match_score)/4\n",
    "\n",
    "        bleu_score = -1\n",
    "        try:\n",
    "            lines = open('../../PLBART/scripts/plbart_csnet/saved_models/'+l1+'-'+l2+'/result.txt', 'r').readlines()\n",
    "            bleu_score = float(lines[1].split(';')[0].split(':')[1])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(l1, l2, '=== CodeBLEU score: ', bleu_score, round(code_bleu_score*100,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## codex-glue StructCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.829, weighted ngram match: 0.8344, syntax_match: 0.9006, dataflow_match: 0.9008\n",
      "java c_sharp === CodeBLEU score:  86.62\n",
      "ngram match: 0.7858, weighted ngram match: 0.7919, syntax_match: 0.9041, dataflow_match: 0.8706\n",
      "c_sharp java === CodeBLEU score:  83.81\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "\n",
    "\n",
    "    \n",
    "for l1 in ['java', 'c_sharp']:\n",
    "    for l2 in ['java', 'c_sharp']:\n",
    "        if l1==l2:\n",
    "            continue\n",
    "    \n",
    "        model_dir = '../saved_models/translation/CodeXGlue/'+l1.replace('c_sharp', 'cs')+'-'+l2.replace('c_sharp', 'cs')+'_27000/'\n",
    "        ref = model_dir + 'test_0.gold'\n",
    "        hyp = model_dir + 'test_0.output'\n",
    "        lang = l2\n",
    "\n",
    "        pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "        hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "        for i in range(len(pre_references)):\n",
    "                assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "        references = []\n",
    "        for i in range(len(hypothesis)):\n",
    "            ref_for_instance = []\n",
    "            for j in range(len(pre_references)):\n",
    "                ref_for_instance.append(pre_references[j][i])\n",
    "            references.append(ref_for_instance)\n",
    "        assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "        # calculate ngram match (BLEU)\n",
    "        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "        # calculate weighted ngram match\n",
    "        keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "        def make_weights(reference_tokens, key_word_list):\n",
    "            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                    for token in reference_tokens}\n",
    "        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "        # calculate syntax match\n",
    "        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "        # calculate dataflow match\n",
    "        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "        print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "                                format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "                                       round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "        code_bleu_score = (ngram_match_score\\\n",
    "                            + weighted_ngram_match_score\\\n",
    "                            + syntax_match_score\\\n",
    "                            + dataflow_match_score)/4\n",
    "\n",
    "        print(l1, l2, '=== CodeBLEU score: ', round(code_bleu_score*100,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### codebleu - cost - baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "java python === CodeBLEU score:  28.85 33.8\n",
      "java c_sharp === CodeBLEU score:  77.74 71.74\n",
      "java javascript === CodeBLEU score:  59.48 54.63\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "java php === CodeBLEU score:  26.35 38.29\n",
      "====================================================================================================\n",
      "python java === CodeBLEU score:  28.91 30.26\n",
      "python c_sharp === CodeBLEU score:  27.93 28.1\n",
      "python javascript === CodeBLEU score:  37.81 38.5\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "python php === CodeBLEU score:  20.35 35.22\n",
      "====================================================================================================\n",
      "c_sharp java === CodeBLEU score:  77.64 74.3\n",
      "c_sharp python === CodeBLEU score:  27.77 32.79\n",
      "c_sharp javascript === CodeBLEU score:  57.86 50.13\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "c_sharp php === CodeBLEU score:  25.89 38.15\n",
      "====================================================================================================\n",
      "javascript java === CodeBLEU score:  59.53 52.61\n",
      "javascript python === CodeBLEU score:  37.81 40.72\n",
      "javascript c_sharp === CodeBLEU score:  58.04 55.62\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "javascript php === CodeBLEU score:  30.74 40.47\n",
      "====================================================================================================\n",
      "php java === CodeBLEU score:  26.98 30.33\n",
      "php python === CodeBLEU score:  20.57 25.85\n",
      "php c_sharp === CodeBLEU score:  26.86 31.37\n",
      "php javascript === CodeBLEU score:  31.19 35.86\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import os\n",
    "\n",
    "model_dir = '../baselines/DOBF/'\n",
    "\n",
    "dir_dict = {'javascript':'Javascript', 'java':'Java', 'c_sharp':'C#', 'php':'PHP', 'python':'Python'}\n",
    "end_dict = {'javascript':'js', 'java':'java', 'c_sharp':'cs', 'php':'php', 'python':'py'}\n",
    "    \n",
    "for l1 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "    print ('='*100)\n",
    "    for l2 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "        if l1==l2:\n",
    "            continue\n",
    "    \n",
    "        ref, hyp = None, None\n",
    "        for filename in os.listdir(model_dir):\n",
    "            if filename.startswith('ref'):\n",
    "                if l1.replace('_','')+'_sa-'+l2.replace('_','')+'_sa' in filename:\n",
    "                    ref = model_dir+filename\n",
    "            if filename.startswith('hyp'):\n",
    "                if l1.replace('_','')+'_sa-'+l2.replace('_','')+'_sa' in filename:\n",
    "                    hyp = model_dir + filename\n",
    "               \n",
    "        try:\n",
    "            lang = l2\n",
    "            pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "            hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "            for i in range(len(pre_references)):\n",
    "                    assert len(hypothesis) == len(pre_references[i])\n",
    "            references = []\n",
    "            for i in range(len(hypothesis)):\n",
    "                ref_for_instance = []\n",
    "                for j in range(len(pre_references)):\n",
    "                    ref_for_instance.append(pre_references[j][i])\n",
    "                references.append(ref_for_instance)\n",
    "            assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "            # calculate ngram match (BLEU)\n",
    "            tokenized_hyps = [x.split() for x in hypothesis]\n",
    "            tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "            ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "            # calculate weighted ngram match\n",
    "            keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "            def make_weights(reference_tokens, key_word_list):\n",
    "                return {token:1 if token in key_word_list else 0.2 \\\n",
    "                        for token in reference_tokens}\n",
    "            tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                        for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "            weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "            # calculate syntax match\n",
    "            syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "            # calculate dataflow match\n",
    "            dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "        #     print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "        #                             format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "        #                                    round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "            code_bleu_score = (ngram_match_score\\\n",
    "                                + weighted_ngram_match_score\\\n",
    "                                + syntax_match_score\\\n",
    "                                + dataflow_match_score)/4\n",
    "\n",
    "    #         print ('ref:', ref)\n",
    "    #         print ('hyp:', hyp)\n",
    "            print(l1, l2, '=== CodeBLEU score: ', round(ngram_match_score*100,2), round(code_bleu_score*100,2))\n",
    "    #         print ('\\n')\n",
    "        except:\n",
    "            print(l1, l2, '=== CodeBLEU score: ___ ____ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## codebleu - naive copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "java python === CodeBLEU score:  30.64 34.98\n",
      "java c_sharp === CodeBLEU score:  77.79 71.39\n",
      "java javascript === CodeBLEU score:  59.79 54.43\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "java php === CodeBLEU score:  26.08 38.14\n",
      ",34.98,71.39,54.43,38.14\n",
      "====================================================================================================\n",
      "python java === CodeBLEU score:  30.69 31.02\n",
      "python c_sharp === CodeBLEU score:  29.77 28.99\n",
      "python javascript === CodeBLEU score:  38.4 38.73\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "python php === CodeBLEU score:  20.06 35.07\n",
      "31.02,,28.99,38.73,35.07\n",
      "====================================================================================================\n",
      "c_sharp java === CodeBLEU score:  77.69 73.76\n",
      "c_sharp python === CodeBLEU score:  29.63 33.91\n",
      "c_sharp javascript === CodeBLEU score:  58.1 50.07\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "c_sharp php === CodeBLEU score:  25.64 38.0\n",
      "73.76,33.91,,50.07,38.0\n",
      "====================================================================================================\n",
      "javascript java === CodeBLEU score:  59.81 51.99\n",
      "javascript python === CodeBLEU score:  38.4 41.14\n",
      "javascript c_sharp === CodeBLEU score:  58.23 54.97\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "javascript php === CodeBLEU score:  29.88 40.04\n",
      "51.99,41.14,54.97,,40.04\n",
      "====================================================================================================\n",
      "php java === CodeBLEU score:  26.67 29.85\n",
      "php python === CodeBLEU score:  20.29 25.86\n",
      "php c_sharp === CodeBLEU score:  26.56 30.99\n",
      "php javascript === CodeBLEU score:  30.37 35.28\n",
      "29.85,25.86,30.99,35.28,\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import os\n",
    "\n",
    "dir_dict = {'javascript':'Javascript', 'java':'Java', 'c_sharp':'C#', 'php':'PHP', 'python':'Python'}\n",
    "end_dict = {'javascript':'js', 'java':'java', 'c_sharp':'cs', 'php':'php', 'python':'py'}\n",
    "    \n",
    "for l1 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "    print ('='*100)\n",
    "    csvline = ''\n",
    "    for l2 in ['java', 'python', 'c_sharp', 'javascript', 'php']:\n",
    "        if l1==l2:\n",
    "            csvline += ','\n",
    "            continue\n",
    "    \n",
    "\n",
    "        ref = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l1]+'-'+dir_dict[l2]+'/' + \\\n",
    "                'test-'+dir_dict[l1]+'-'+dir_dict[l2]+'-tok.'+end_dict[l2]\n",
    "        hyp = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l1]+'-'+dir_dict[l2]+'/' + \\\n",
    "                'test-'+dir_dict[l1]+'-'+dir_dict[l2]+'-tok.'+end_dict[l1]\n",
    "        if not(os.path.exists(ref)):\n",
    "            ref = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l2]+'-'+dir_dict[l1]+'/' + \\\n",
    "                'test-'+dir_dict[l2]+'-'+dir_dict[l1]+'-tok.'+end_dict[l2]\n",
    "            hyp = '/home/tsaisindhura/datasets/g4g/pair_data_tok_full/'+dir_dict[l2]+'-'+dir_dict[l1]+'/' + \\\n",
    "                'test-'+dir_dict[l2]+'-'+dir_dict[l1]+'-tok.'+end_dict[l1]\n",
    "        \n",
    "        lang = l2\n",
    "        \n",
    "\n",
    "        pre_references = [[x.strip() for x in open(ref, 'r', encoding='utf-8').readlines()]]\n",
    "        hypothesis = [x.strip() for x in open(hyp, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "        for i in range(len(pre_references)):\n",
    "                assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "        references = []\n",
    "        for i in range(len(hypothesis)):\n",
    "            ref_for_instance = []\n",
    "            for j in range(len(pre_references)):\n",
    "                ref_for_instance.append(pre_references[j][i])\n",
    "            references.append(ref_for_instance)\n",
    "        assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "        # calculate ngram match (BLEU)\n",
    "        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "        # calculate weighted ngram match\n",
    "        keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "        def make_weights(reference_tokens, key_word_list):\n",
    "            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                    for token in reference_tokens}\n",
    "        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "        # calculate syntax match\n",
    "        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "        # calculate dataflow match\n",
    "        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "    #     print('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "    #                             format(round(ngram_match_score, 4), round(weighted_ngram_match_score, 4), \\\n",
    "    #                                    round(syntax_match_score, 4), round(dataflow_match_score,4)))\n",
    "\n",
    "        code_bleu_score = (ngram_match_score\\\n",
    "                            + weighted_ngram_match_score\\\n",
    "                            + syntax_match_score\\\n",
    "                            + dataflow_match_score)/4\n",
    "        \n",
    "        print(l1, l2, '=== CodeBLEU score: ', round(ngram_match_score*100,2), round(code_bleu_score*100,2))\n",
    "        \n",
    "        csvline += str(round(code_bleu_score*100,2))+','\n",
    "        \n",
    "    print (csvline[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
